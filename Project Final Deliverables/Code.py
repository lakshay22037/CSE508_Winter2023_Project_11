# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YSyMI_XVhL2YrUKzfbork-ruBLUOzurg
"""

# pip install gtts

# !wget "https://pjreddie.com/media/files/yolov3.weights"

# !wget "https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3.cfg"

# !wget "https://raw.githubusercontent.com/pjreddie/darknet/master/data/coco.names"

import cv2
import numpy as np
from gtts import gTTS
from IPython.display import Audio, display_html
from IPython.display import display

# Load the YOLOv3 object detection model
net = cv2.dnn.readNetFromDarknet('yolov3.cfg', 'yolov3.weights')

# Load the COCO class names
classes = []
with open('coco.names', 'r') as f:
    classes = [line.strip() for line in f.readlines()]

# Define the names of the output layers
output_layers = net.getUnconnectedOutLayersNames()

# Define the minimum confidence threshold for detections
conf_threshold = 0.3

# Define the non-maximum suppression threshold for overlapping boxes
nms_threshold = 0.3

def predict(image_path, id):
    if(id==0):
        #Input Image
        # Load the image
        img = cv2.imread(image_path)

        if img is None:
            return "No image with this name found!"

        # Increase the image size
        img = cv2.resize(img, (608, 608))

        # Preprocess the image
        blob = cv2.dnn.blobFromImage(img, 1/255.0, (608, 608), swapRB=True, crop=False)

        # Pass the image through the network
        net.setInput(blob)
        outs = net.forward(output_layers)

        # Extract the object labels and bounding boxes from the network outputs
        class_ids = []
        confidences = []
        boxes = []
        for out in outs:
            for detection in out:
                scores = detection[5:]
                class_id = np.argmax(scores)
                confidence = scores[class_id]
                if confidence > conf_threshold:
                    center_x = int(detection[0] * img.shape[1])
                    center_y = int(detection[1] * img.shape[0])
                    width = int(detection[2] * img.shape[1])
                    height = int(detection[3] * img.shape[0])
                    left = int(center_x - width / 2)
                    top = int(center_y - height / 2)
                    class_ids.append(class_id)
                    confidences.append(float(confidence))
                    boxes.append([left, top, width, height])

        # Apply non-maximum suppression to remove overlapping boxes
        indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)
        labels = []
        for i in indices:
            i = int(i)
            label = class_ids[i]
            if label not in labels:
                labels.append(label)

        # Print the object labels
        label_name = []
        if len(indices) > 0:
            print('This image contains the following objects:')
            for i in indices.flatten():
                label = str(classes[class_ids[i]])
                if label not in label_name:
                    # print(label)
                    label_name.append(label)
        else:
            print('No objects were detected in this image.')

    
        # Generate the audio description
        if len(indices) != 0:
            description = 'This image contains '
            for i, label in enumerate(label_name):
                if i == 0:
                    description += label
                elif i == len(labels) - 1:
                    description += ' and ' + label
                else:
                    description += ', ' + label

            # Convert the description to speech and save it
            tts = gTTS(description)
            tts.save('static/description.mp3')
            return label_name, description
        else:
            return [], 'No objects were detected in this image.'
    else:
        #Relevant Images
        rel_labels=[]
        rel_desc=[]
        for p in range(len(image_path)):
            
            # Load the image
            img = cv2.imread(image_path[p])

            if img is None:
                return "No image with this name found!"

            # Increase the image size
            img = cv2.resize(img, (608, 608))

            # Preprocess the image
            blob = cv2.dnn.blobFromImage(img, 1/255.0, (608, 608), swapRB=True, crop=False)

            # Pass the image through the network
            net.setInput(blob)
            outs = net.forward(output_layers)

            # Extract the object labels and bounding boxes from the network outputs
            class_ids = []
            confidences = []
            boxes = []
            for out in outs:
                for detection in out:
                    scores = detection[5:]
                    class_id = np.argmax(scores)
                    confidence = scores[class_id]
                    if confidence > conf_threshold:
                        center_x = int(detection[0] * img.shape[1])
                        center_y = int(detection[1] * img.shape[0])
                        width = int(detection[2] * img.shape[1])
                        height = int(detection[3] * img.shape[0])
                        left = int(center_x - width / 2)
                        top = int(center_y - height / 2)
                        class_ids.append(class_id)
                        confidences.append(float(confidence))
                        boxes.append([left, top, width, height])

            # Apply non-maximum suppression to remove overlapping boxes
            indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)
            labels = []
            for i in indices:
                i = int(i)
                label = class_ids[i]
                if label not in labels:
                    labels.append(label)

            # Print the object labels
            label_name = []
            if len(indices) > 0:
                print('This image contains the following objects:')
                for i in indices.flatten():
                    label = str(classes[class_ids[i]])
                    if label not in label_name:
                        # print(label)
                        label_name.append(label)
            else:
                print('No objects were detected in this image.')

        
            # Generate the audio description
            description='No objects were detected in this image.'
            if len(indices) != 0:
                description = 'This image contains '
                for i, label in enumerate(label_name):
                    if i == 0:
                        description += label
                    elif i == len(labels) - 1:
                        description += ' and ' + label
                    else:
                        description += ', ' + label

                rel_labels.append(label_name)
                rel_desc.append(description)
            else:
                rel_labels.append([])
                rel_desc.append('No objects were detected in this image.')
            # Convert the description to speech and save it
            tts = gTTS(description)
            tts.save('static/description'+str(p+1)+'.mp3')
        return rel_labels,rel_desc